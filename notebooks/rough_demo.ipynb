{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9  ('qualia': venv)",
   "metadata": {
    "interpreter": {
     "hash": "9f216fef417bc6c42f31995dd72061a05be77532af677c96d7eb6f97a46421b7"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/mnt/z/Project/semantic_search/qualia\n"
     ]
    }
   ],
   "source": [
    "%cd /mnt/z/Project/semantic_search/qualia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import transformers\n",
    "from embedding_model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/mnt/z/Project/semantic_search/qualia\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model Compiled: stsb-distilbert-base\n"
     ]
    }
   ],
   "source": [
    "m.make_pretrained('stsb-distilbert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset\n",
    "# Big JSON file \n",
    "import json\n",
    "arxiv = json.load(open('/mnt/z/Project/semantic_search/qualia/data/arxivData.json'))\n",
    "# arxiv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_ds(x, useful_keys):\n",
    "    return '. '.join([x[useful_keys[0]], x[useful_keys[1]]])\n",
    "\n",
    "useful_keys = ['title', 'summary']\n",
    "arxiv_data = list(map(lambda x: join_ds(x, useful_keys), arxiv))\n",
    "arxiv_data_labels = list(range(len(arxiv_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = dict(zip(arxiv_data_labels, arxiv_data))\n",
    "# label_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(200, 768)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "sample_len = 200\n",
    "data_vec = m.encode_sentences(arxiv_data[:sample_len])\n",
    "data_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = arxiv_data_labels[:sample_len]\n",
    "# data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Index from docs\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import hnswlib\n",
    "import numpy as np\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "HNSW_PARAMS = {\n",
    "    \"save_file\": 'hnsw_index.bin',\n",
    "    \"M\": 16,\n",
    "    \"ef_construction\": 200,\n",
    "    \"num_threads\": MAX_SEARCH_THREADS,\n",
    "    \"num_elements\": 50,\n",
    "    \"label_mapping\": label_mapping \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from search_index import Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hnsw_index = Index(HNSW_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "hnsw_index.M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "First time?\nSaving Fresh New index: /mnt/z/Project/semantic_search/qualia/model/v1/hnsw_index/hnsw_index.bin\nSave index size: 10\n"
     ]
    }
   ],
   "source": [
    "hnsw_index.define_index(idx_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n            Loading Previously saved index:\n                Loading File:     hnsw_index.bin\n                New max_elements: 410\n        \nAdding n=10 batches of items from the data\n200 200\nAdding keys to update list\nAdding keys to update list\nAdding keys to update list\nAdding keys to update list\nAdding keys to update list\nAdding keys to update list\nAdding keys to update list\nAdding keys to update list\nAdding keys to update list\nAdding keys to update list\nAdding keys to update list\nAdding keys to update list\nAdding keys to update list\nAdding keys to update list\nAdding keys to update list\nAdding keys to update list\nAdding keys to update list\nAdding keys to update list\nAdding keys to update list\nAdding keys to update list\nSaving new index of size 200\nSave index size: 200\n"
     ]
    }
   ],
   "source": [
    "hnsw_index.update_index(data_vec, data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(hnsw_index.index.get_ids_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading saved index\n"
     ]
    }
   ],
   "source": [
    "hnsw_index.init_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'matches': [{81: ('A Structured Self-attentive Sentence Embedding. This paper proposes a new model for extracting an interpretable sentence\\nembedding by introducing self-attention. Instead of using a vector, we use a\\n2-D matrix to represent the embedding, with each row of the matrix attending on\\na different part of the sentence. We also propose a self-attention mechanism\\nand a special regularization term for the model. As a side effect, the\\nembedding comes with an easy way of visualizing what specific parts of the\\nsentence are encoded into the embedding. We evaluate our model on 3 different\\ntasks: author profiling, sentiment classification, and textual entailment.\\nResults show that our model yields a significant performance gain compared to\\nother sentence embedding methods in all of the 3 tasks.',\n",
       "    '45.662')},\n",
       "  {48: ('Explaining Recurrent Neural Network Predictions in Sentiment Analysis. Recently, a technique called Layer-wise Relevance Propagation (LRP) was shown\\nto deliver insightful explanations in the form of input space relevances for\\nunderstanding feed-forward neural network classification decisions. In the\\npresent work, we extend the usage of LRP to recurrent neural networks. We\\npropose a specific propagation rule applicable to multiplicative connections as\\nthey arise in recurrent network architectures such as LSTMs and GRUs. We apply\\nour technique to a word-based bi-directional LSTM model on a five-class\\nsentiment prediction task, and evaluate the resulting LRP relevances both\\nqualitatively and quantitatively, obtaining better results than a\\ngradient-based related method which was used in previous work.',\n",
       "    '40.013')}]}"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "# hnsw_index.k = 2\n",
    "MAX_NEAREST_NBRS = 2\n",
    "hnsw_index.search(m.encode_sentences(\"A Structured Self-attentive Sentence Embedding\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "hnsw_index.M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Index():\n",
    "    def __init__(self, hnsw_params:dict):\n",
    "        \"\"\"\n",
    "        {\n",
    "            \"save_dir\": ,\n",
    "            \"save_file\": ,\n",
    "            \"M\": ,\n",
    "            \"ef_construction\": ,\n",
    "            \"item_batch_size\": ,\n",
    "            \"num_threads\": ,\n",
    "            \"num_elements\": \n",
    "        }\n",
    "        \"\"\"\n",
    "        self.SAVE_DIR = hnsw_params.get(\"save_dir\", INDEX_DIR)\n",
    "        self.SAVE_FILE = hnsw_params.get(\"save_file\", \"hnsw_index.bin\")\n",
    "        self.CURR_IDX_SIZE = self.find_current_index_size()\n",
    "        self.M = hnsw_params.get(\"M\", 200)\n",
    "        self.ef_construction = hnsw_params.get(\"ef_construction\", 200)\n",
    "        self.item_batch_size = hnsw_params.get(\"item_batch_size\", 10)\n",
    "        self.num_threads = hnsw_params.get(\"num_threads\", MAX_SEARCH_THREADS)\n",
    "        self.num_elements = hnsw_params.get(\"num_elements\", None)\n",
    "        self.label_mapping = hnsw_params.get(\"label_mapping\", None)\n",
    "        self.index = hnswlib.Index(space='cosine', dim=EMB_DIM)\n",
    "        self.index_loaded = False\n",
    "\n",
    "    def init_search(self):\n",
    "        \"\"\"\n",
    "        load any presaved index if not, return empty index def\n",
    "        \"\"\"\n",
    "        print(\"Loading saved index\")\n",
    "        self.index.load_index(self.SAVE_DIR+self.SAVE_FILE,\n",
    "                              max_elements=self.CURR_IDX_SIZE)\n",
    "        self.index_loaded = True\n",
    "\n",
    "    def find_current_index_size(self):\n",
    "        \"\"\"\n",
    "            Find saved index size \n",
    "        \"\"\"\n",
    "        try:\n",
    "            return pickle.load(open(INDEX_SIZE_PATH, 'rb'))\n",
    "        except Exception as e:\n",
    "            print(\"No Index size pre saved\")\n",
    "            return None\n",
    "\n",
    "    def define_index(self, idx_size):\n",
    "        \"\"\"\n",
    "        define fresh new index with params given when init\n",
    "        save in given path\n",
    "        \"\"\"\n",
    "        print(\"First time?\")\n",
    "        # Initing index - the maximum number of elements should be known beforehand\n",
    "        self.CURR_IDX_SIZE = idx_size\n",
    "        self.index.init_index(max_elements=self.CURR_IDX_SIZE,\n",
    "                              ef_construction=self.ef_construction,\n",
    "                              M=self.M)\n",
    "        print(f\"Saving Fresh New index: {self.SAVE_DIR+self.SAVE_FILE}\")\n",
    "        self.index.save_index(self.SAVE_DIR+self.SAVE_FILE)\n",
    "        print(f\"Save index size: {self.CURR_IDX_SIZE}\")\n",
    "        pickle.dump(self.CURR_IDX_SIZE, open(INDEX_SIZE_PATH, 'wb'))\n",
    "\n",
    "    def _batch(self, iterable1, iterable2, n=1):\n",
    "        l1 = iterable1.shape[0]\n",
    "        l2 = iterable2.shape[0]\n",
    "        print(l1, l2)\n",
    "        assert l1 == l2\n",
    "        # print(\"LEN OF ITER\", l)\n",
    "        for ndx in range(0, l1, n):\n",
    "            # print(\"ITER\", iterable[ndx:min(ndx + n, l)])\n",
    "            yield iterable1[ndx:min(ndx + n, l1)], iterable2[ndx:min(ndx + n, l1)]\n",
    "    \n",
    "    def get_current_count(self):\n",
    "        if not self.index_loaded:\n",
    "            print(\"Index Not Loaded, please run init_search() first\")\n",
    "            return False\n",
    "        return self.index.get_current_count()\n",
    "\n",
    "\n",
    "    def update_index(self, data, data_labels):\n",
    "        \"\"\"\n",
    "        batch add things to index and save to save path\n",
    "        data should be:\n",
    "            [(vector, label)]\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.index_loaded:\n",
    "            print(\"Index Not Loaded, please run init_search() first\")\n",
    "            return False\n",
    "                \n",
    "        print(f\"\"\"\n",
    "            Loading Previously saved index:\n",
    "                Loading File:     {self.SAVE_FILE}\n",
    "                New max_elements: {self.CURR_IDX_SIZE + len(data)}\n",
    "        \"\"\")\n",
    "        try:\n",
    "            num_elements = len(data)\n",
    "            self.index.load_index(self.SAVE_DIR+self.SAVE_FILE,\n",
    "                                  max_elements=self.CURR_IDX_SIZE+num_elements)\n",
    "            self.CURR_IDX_SIZE += num_elements\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Probably not saved earlier\")\n",
    "            return None\n",
    "        \n",
    "        # List of qids to update as 'processed=True'\n",
    "        update_list = [] \n",
    "        data = np.array(data)  \n",
    "        data_labels = np.array(data_labels) \n",
    "        # Element insertion (can be called several times):\n",
    "        print(f\"Adding n={self.item_batch_size} batches of items from the data\")\n",
    "        for d, dl in self._batch(data, data_labels, n=self.item_batch_size):\n",
    "            self.index.add_items(data=d,\n",
    "                                 ids=dl,\n",
    "                                 num_threads=self.num_threads)\n",
    "            print(f\"Adding keys to update list\")\n",
    "            update_list.extend(dl)\n",
    "\n",
    "        print(f\"Saving new index of size {self.index.get_current_count()}\")\n",
    "        \n",
    "        # Save index so far\n",
    "        self.index.save_index(self.SAVE_DIR+self.SAVE_FILE)\n",
    "        \n",
    "        self.CURR_IDX_SIZE = self.index.get_current_count()\n",
    "        print(f\"Save index size: {self.CURR_IDX_SIZE}\")\n",
    "        pickle.dump(self.CURR_IDX_SIZE, open(INDEX_SIZE_PATH, 'wb'))\n",
    "\n",
    "    def search(self, query_vector):\n",
    "        if not self.index_loaded:\n",
    "            print(\"Index Not Loaded, please run init_search() first\")\n",
    "            return False\n",
    "\n",
    "        # vec = vectorizer.transform([query])\n",
    "        # vec = [svd.transform(x.A)[0] for x in vec]\n",
    "        labels, distances = self.index.knn_query(list(query_vector),\n",
    "                                                 k=MAX_NEAREST_NBRS,\n",
    "                                                 num_threads=self.num_threads)\n",
    "        if labels is None:\n",
    "            print(f\"No Matches found\")\n",
    "            return {\"matches\": []}\n",
    "\n",
    "        if self.label_mapping is not None:\n",
    "            resp = []\n",
    "            for q_ids, dists in zip(labels, distances):\n",
    "                raw_set = []\n",
    "                for qid, d in zip(q_ids, dists):\n",
    "                    raw_set.append((int(qid), d, self.label_mapping[qid]))\n",
    "                raw_set = sorted(raw_set, key=lambda x: x[1], reverse=False)\n",
    "                resp.append([{k: (v, \"%.3f\"%((1-d)*100))} for k, d, v in raw_set])\n",
    "            \n",
    "\n",
    "        return {\"matches\": resp[0]}\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "MAX_NEAREST_NBRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}