{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythonjvsc74a57bd09f216fef417bc6c42f31995dd72061a05be77532af677c96d7eb6f97a46421b7",
   "display_name": "Python 3.6.9  ('qualia': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "9f216fef417bc6c42f31995dd72061a05be77532af677c96d7eb6f97a46421b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import transformers\n",
    "from embedding_model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model Compiled: stsb-distilbert-base\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained Model\n",
    "model = Model()\n",
    "model.make_pretrained('stsb-distilbert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(['MIT at SemEval-2017 Task 10: Relation Extraction with Convolutional\\n  Neural Networks. Over 50 million scholarly articles have been published: they constitute a\\nunique repository of knowledge. In particular, one may infer from them\\nrelations between scientific concepts, such as synonyms and hyponyms.\\nArtificial neural networks have been recently explored for relation extraction.\\nIn this work, we continue this line of work and present a system based on a\\nconvolutional neural network to extract relations. Our model ranked first in\\nthe SemEval-2017 task 10 (ScienceIE) for relation extraction in scientific\\narticles (subtask C).',\n",
       "  'Transfer Learning for Named-Entity Recognition with Neural Networks. Recent approaches based on artificial neural networks (ANNs) have shown\\npromising results for named-entity recognition (NER). In order to achieve high\\nperformances, ANNs need to be trained on a large labeled dataset. However,\\nlabels might be difficult to obtain for the dataset on which the user wants to\\nperform NER: label scarcity is particularly pronounced for patient note\\nde-identification, which is an instance of NER. In this work, we analyze to\\nwhat extent transfer learning may address this issue. In particular, we\\ndemonstrate that transferring an ANN model trained on a large labeled dataset\\nto another dataset with a limited number of labels improves upon the\\nstate-of-the-art results on two different datasets for patient note\\nde-identification.'],\n",
       " [45, 46])"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# Load the sample Dataset\n",
    "from sample_dataset import SampleDataset\n",
    "arxiv_data = SampleDataset()\n",
    "arxiv_data.load()\n",
    "\n",
    "# Dataset is a sample of arXiv paper extracts and a \n",
    "# corresponding numeric label used internally \n",
    "# Taking a look \n",
    "arxiv_data[45:47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data to be added to the index:  300 (300, 768)\n"
     ]
    }
   ],
   "source": [
    "# Turn documents into vectors of dim (sample_len, 768)\n",
    "# Subselecting a few samples, \n",
    "i = 0\n",
    "sample_len = 1000\n",
    "\n",
    "data, data_labels = arxiv_data[i:sample_len]\n",
    "\n",
    "data_vec = model.encode_sentences(data)\n",
    "print(\"Data to be added to the index: \", len(data_labels), data_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Index from docs\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import hnswlib\n",
    "import numpy as np\n",
    "from utils import *\n",
    "\n",
    "# Parameters to initiate the HNSW Index\n",
    "HNSW_PARAMS = {\n",
    "    \"save_file\": 'hnsw_index.bin',\n",
    "    \"M\": 200,\n",
    "    \"ef_construction\": 200,\n",
    "    \"num_threads\": MAX_SEARCH_THREADS,\n",
    "    \"label_mapping\": arxiv_data.label_mapping \n",
    "}\n",
    "\n",
    "from search_index import Index\n",
    "\n",
    "# Initiate the Search Index wrapper class \n",
    "hnsw_index = Index(HNSW_PARAMS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n        HNSW Index Params: \n            SAVE_DIR: model/v1/hnsw_index/,\n            SAVE_FILE: hnsw_index.bin,\n            CURR_IDX_SIZE: 10,\n            M: 200,\n            ef_construction: 200,\n            item_batch_size: 10,\n            num_threads: -1,\n            index_loaded: False\n        \n"
     ]
    }
   ],
   "source": [
    "print(hnsw_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "First time?\nSaving Fresh New index: model/v1/hnsw_index/hnsw_index.bin\nSave index size: 10\nLoading saved index\n"
     ]
    }
   ],
   "source": [
    "# Create a new index with the given params and save it to given file \n",
    "hnsw_index.define_index(idx_size=10)\n",
    "hnsw_index.init_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "            Loading Previously saved index:\n",
      "                Loading File:     hnsw_index.bin\n",
      "                New max_elements: 600\n",
      "        \n",
      "Adding n=10 batches of items from the data\n",
      "Saving new index of size 300\n",
      "Save index size: 300\n"
     ]
    }
   ],
   "source": [
    "# Add the data to the index\n",
    "hnsw_index.update_index(data_vec, data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NEAREST_NBRS = 15\n",
    "query_text = \"representation learning\"\n",
    "matches = hnsw_index.search(\n",
    "            model.encode_sentences(\n",
    "                query_text\n",
    "                ),\n",
    "            max_nearest_nbrs=MAX_NEAREST_NBRS\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "hnsw_index.get_current_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "len(matches['matches'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Matches to Query: representation learning\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{146: ('Learning unbiased features. A key element in transfer learning is representation learning; if\\nrepresentations can be developed that expose the relevant factors underlying\\nthe data, then new tasks and domains can be learned readily based on mappings\\nof these salient factors. We propose that an important aim for these\\nrepresentations are to be unbiased. Different forms of representation learning\\ncan be derived from alternative definitions of unwanted bias, e.g., bias to\\nparticular tasks, domains, or irrelevant underlying data dimensions. One very\\nuseful approach to estimating the amount of bias in a representation comes from\\nmaximum mean discrepancy (MMD) [5], a measure of distance between probability\\ndistributions. We are not the first to suggest that MMD can be a useful\\ncriterion in developing representations that apply across multiple domains or\\ntasks [1]. However, in this paper we describe a number of novel applications of\\nthis criterion that we have devised, all based on the idea of developing\\nunbiased representations. These formulations include: a standard domain\\nadaptation framework; a method of learning invariant representations; an\\napproach based on noise-insensitive autoencoders; and a novel form of\\ngenerative model.',\n",
       "   '58.680')},\n",
       " {87: ('Object-oriented Neural Programming (OONP) for Document Understanding. We propose Object-oriented Neural Programming (OONP), a framework for\\nsemantically parsing documents in specific domains. Basically, OONP reads a\\ndocument and parses it into a predesigned object-oriented data structure\\n(referred to as ontology in this paper) that reflects the domain-specific\\nsemantics of the document. An OONP parser models semantic parsing as a decision\\nprocess: a neural net-based Reader sequentially goes through the document, and\\nduring the process it builds and updates an intermediate ontology to summarize\\nits partial understanding of the text it covers. OONP supports a rich family of\\noperations (both symbolic and differentiable) for composing the ontology, and a\\nbig variety of forms (both symbolic and differentiable) for representing the\\nstate and the document. An OONP parser can be trained with supervision of\\ndifferent forms and strength, including supervised learning (SL) ,\\nreinforcement learning (RL) and hybrid of the two. Our experiments on both\\nsynthetic and real-world document parsing tasks have shown that OONP can learn\\nto handle fairly complicated ontology with training data of modest sizes.',\n",
       "   '57.243')},\n",
       " {96: ('Learning Convolutional Text Representations for Visual Question\\n  Answering. Visual question answering is a recently proposed artificial intelligence task\\nthat requires a deep understanding of both images and texts. In deep learning,\\nimages are typically modeled through convolutional neural networks, and texts\\nare typically modeled through recurrent neural networks. While the requirement\\nfor modeling images is similar to traditional computer vision tasks, such as\\nobject recognition and image classification, visual question answering raises a\\ndifferent need for textual representation as compared to other natural language\\nprocessing tasks. In this work, we perform a detailed analysis on natural\\nlanguage questions in visual question answering. Based on the analysis, we\\npropose to rely on convolutional neural networks for learning textual\\nrepresentations. By exploring the various properties of convolutional neural\\nnetworks specialized for text data, such as width and depth, we present our\\n\"CNN Inception + Gate\" model. We show that our model improves question\\nrepresentations and thus the overall accuracy of visual question answering\\nmodels. We also show that the text representation requirement in visual\\nquestion answering is more complicated and comprehensive than that in\\nconventional natural language processing tasks, making it a better task to\\nevaluate textual representation methods. Shallow models like fastText, which\\ncan obtain comparable results with deep learning models in tasks like text\\nclassification, are not suitable in visual question answering.',\n",
       "   '56.105')},\n",
       " {270: ('Toward Controlled Generation of Text. Generic generation and manipulation of text is challenging and has limited\\nsuccess compared to recent deep generative modeling in visual domain. This\\npaper aims at generating plausible natural language sentences, whose attributes\\nare dynamically controlled by learning disentangled latent representations with\\ndesignated semantics. We propose a new neural generative model which combines\\nvariational auto-encoders and holistic attribute discriminators for effective\\nimposition of semantic structures. With differentiable approximation to\\ndiscrete text samples, explicit constraints on independent attribute controls,\\nand efficient collaborative learning of generator and discriminators, our model\\nlearns highly interpretable representations from even only word annotations,\\nand produces realistic sentences with desired attributes. Quantitative\\nevaluation validates the accuracy of sentence and attribute generation.',\n",
       "   '55.580')},\n",
       " {111: ('A Latent Variable Recurrent Neural Network for Discourse Relation\\n  Language Models. This paper presents a novel latent variable recurrent neural network\\narchitecture for jointly modeling sequences of words and (possibly latent)\\ndiscourse relations between adjacent sentences. A recurrent neural network\\ngenerates individual words, thus reaping the benefits of\\ndiscriminatively-trained vector representations. The discourse relations are\\nrepresented with a latent variable, which can be predicted or marginalized,\\ndepending on the task. The resulting model can therefore employ a training\\nobjective that includes not only discourse relation classification, but also\\nword prediction. As a result, it outperforms state-of-the-art alternatives for\\ntwo tasks: implicit discourse relation classification in the Penn Discourse\\nTreebank, and dialog act classification in the Switchboard corpus. Furthermore,\\nby marginalizing over latent discourse relations at test time, we obtain a\\ndiscourse informed language model, which improves over a strong LSTM baseline.',\n",
       "   '54.761')},\n",
       " {81: ('A Structured Self-attentive Sentence Embedding. This paper proposes a new model for extracting an interpretable sentence\\nembedding by introducing self-attention. Instead of using a vector, we use a\\n2-D matrix to represent the embedding, with each row of the matrix attending on\\na different part of the sentence. We also propose a self-attention mechanism\\nand a special regularization term for the model. As a side effect, the\\nembedding comes with an easy way of visualizing what specific parts of the\\nsentence are encoded into the embedding. We evaluate our model on 3 different\\ntasks: author profiling, sentiment classification, and textual entailment.\\nResults show that our model yields a significant performance gain compared to\\nother sentence embedding methods in all of the 3 tasks.',\n",
       "   '53.782')},\n",
       " {260: ('Gaussian Attention Model and Its Application to Knowledge Base Embedding\\n  and Question Answering. We propose the Gaussian attention model for content-based neural memory\\naccess. With the proposed attention model, a neural network has the additional\\ndegree of freedom to control the focus of its attention from a laser sharp\\nattention to a broad attention. It is applicable whenever we can assume that\\nthe distance in the latent space reflects some notion of semantics. We use the\\nproposed attention model as a scoring function for the embedding of a knowledge\\nbase into a continuous vector space and then train a model that performs\\nquestion answering about the entities in the knowledge base. The proposed\\nattention model can handle both the propagation of uncertainty when following a\\nseries of relations and also the conjunction of conditions in a natural way. On\\na dataset of soccer players who participated in the FIFA World Cup 2014, we\\ndemonstrate that our model can handle both path queries and conjunctive queries\\nwell.',\n",
       "   '53.660')},\n",
       " {130: ('Deep Embedding for Spatial Role Labeling. This paper introduces the visually informed embedding of word (VIEW), a\\ncontinuous vector representation for a word extracted from a deep neural model\\ntrained using the Microsoft COCO data set to forecast the spatial arrangements\\nbetween visual objects, given a textual description. The model is composed of a\\ndeep multilayer perceptron (MLP) stacked on the top of a Long Short Term Memory\\n(LSTM) network, the latter being preceded by an embedding layer. The VIEW is\\napplied to transferring multimodal background knowledge to Spatial Role\\nLabeling (SpRL) algorithms, which recognize spatial relations between objects\\nmentioned in the text. This work also contributes with a new method to select\\ncomplementary features and a fine-tuning method for MLP that improves the $F1$\\nmeasure in classifying the words into spatial roles. The VIEW is evaluated with\\nthe Task 3 of SemEval-2013 benchmark data set, SpaceEval.',\n",
       "   '53.248')},\n",
       " {95: ('Discourse-Based Objectives for Fast Unsupervised Sentence Representation\\n  Learning. This work presents a novel objective function for the unsupervised training\\nof neural network sentence encoders. It exploits signals from paragraph-level\\ndiscourse coherence to train these models to understand text. Our objective is\\npurely discriminative, allowing us to train models many times faster than was\\npossible under prior methods, and it yields models which perform well in\\nextrinsic evaluations.',\n",
       "   '52.870')},\n",
       " {69: ('Neural Enquirer: Learning to Query Tables with Natural Language. We proposed Neural Enquirer as a neural network architecture to execute a\\nnatural language (NL) query on a knowledge-base (KB) for answers. Basically,\\nNeural Enquirer finds the distributed representation of a query and then\\nexecutes it on knowledge-base tables to obtain the answer as one of the values\\nin the tables. Unlike similar efforts in end-to-end training of semantic\\nparsers, Neural Enquirer is fully \"neuralized\": it not only gives\\ndistributional representation of the query and the knowledge-base, but also\\nrealizes the execution of compositional queries as a series of differentiable\\noperations, with intermediate results (consisting of annotations of the tables\\nat different levels) saved on multiple layers of memory. Neural Enquirer can be\\ntrained with gradient descent, with which not only the parameters of the\\ncontrolling components and semantic parsing component, but also the embeddings\\nof the tables and query words can be learned from scratch. The training can be\\ndone in an end-to-end fashion, but it can take stronger guidance, e.g., the\\nstep-by-step supervision for complicated queries, and benefit from it. Neural\\nEnquirer is one step towards building neural network systems which seek to\\nunderstand language by executing it on real-world. Our experiments show that\\nNeural Enquirer can learn to execute fairly complicated NL queries on tables\\nwith rich structures.',\n",
       "   '52.408')},\n",
       " {15: (\"Learning Features by Watching Objects Move. This paper presents a novel yet intuitive approach to unsupervised feature\\nlearning. Inspired by the human visual system, we explore whether low-level\\nmotion-based grouping cues can be used to learn an effective visual\\nrepresentation. Specifically, we use unsupervised motion-based segmentation on\\nvideos to obtain segments, which we use as 'pseudo ground truth' to train a\\nconvolutional network to segment objects from a single frame. Given the\\nextensive evidence that motion plays a key role in the development of the human\\nvisual system, we hope that this straightforward approach to unsupervised\\nlearning will be more effective than cleverly designed 'pretext' tasks studied\\nin the literature. Indeed, our extensive experiments show that this is the\\ncase. When used for transfer learning on object detection, our representation\\nsignificantly outperforms previous unsupervised approaches across multiple\\nsettings, especially when training data for the target task is scarce.\",\n",
       "   '51.878')},\n",
       " {74: ('A Hierarchical Latent Variable Encoder-Decoder Model for Generating\\n  Dialogues. Sequential data often possesses a hierarchical structure with complex\\ndependencies between subsequences, such as found between the utterances in a\\ndialogue. In an effort to model this kind of generative process, we propose a\\nneural network-based generative architecture, with latent stochastic variables\\nthat span a variable number of time steps. We apply the proposed model to the\\ntask of dialogue response generation and compare it with recent neural network\\narchitectures. We evaluate the model performance through automatic evaluation\\nmetrics and by carrying out a human evaluation. The experiments demonstrate\\nthat our model improves upon recently proposed models and that the latent\\nvariables facilitate the generation of long outputs and maintain the context.',\n",
       "   '51.784')},\n",
       " {208: ('Self-informed neural network structure learning. We study the problem of large scale, multi-label visual recognition with a\\nlarge number of possible classes. We propose a method for augmenting a trained\\nneural network classifier with auxiliary capacity in a manner designed to\\nsignificantly improve upon an already well-performing model, while minimally\\nimpacting its computational footprint. Using the predictions of the network\\nitself as a descriptor for assessing visual similarity, we define a\\npartitioning of the label space into groups of visually similar entities. We\\nthen augment the network with auxilliary hidden layer pathways with\\nconnectivity only to these groups of label units. We report a significant\\nimprovement in mean average precision on a large-scale object recognition task\\nwith the augmented model, while increasing the number of multiply-adds by less\\nthan 3%.',\n",
       "   '51.557')},\n",
       " {8: ('Tutorial on Answering Questions about Images with Deep Learning. Together with the development of more accurate methods in Computer Vision and\\nNatural Language Understanding, holistic architectures that answer on questions\\nabout the content of real-world images have emerged. In this tutorial, we build\\na neural-based approach to answer questions about images. We base our tutorial\\non two datasets: (mostly on) DAQUAR, and (a bit on) VQA. With small tweaks the\\nmodels that we present here can achieve a competitive performance on both\\ndatasets, in fact, they are among the best methods that use a combination of\\nLSTM with a global, full frame CNN representation of an image. We hope that\\nafter reading this tutorial, the reader will be able to use Deep Learning\\nframeworks, such as Keras and introduced Kraino, to build various architectures\\nthat will lead to a further performance improvement on this challenging task.',\n",
       "   '50.665')},\n",
       " {16: ('Domain Adaptive Neural Networks for Object Recognition. We propose a simple neural network model to deal with the domain adaptation\\nproblem in object recognition. Our model incorporates the Maximum Mean\\nDiscrepancy (MMD) measure as a regularization in the supervised learning to\\nreduce the distribution mismatch between the source and target domains in the\\nlatent space. From experiments, we demonstrate that the MMD regularization is\\nan effective tool to provide good domain adaptation models on both SURF\\nfeatures and raw image pixels of a particular image data set. We also show that\\nour proposed model, preceded by the denoising auto-encoder pretraining,\\nachieves better performance than recent benchmark models on the same data sets.\\nThis work represents the first study of MMD measure in the context of neural\\nnetworks.',\n",
       "   '50.544')}]"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "print(f\"Matches to Query: {query_text}\")\n",
    "matches['matches']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}